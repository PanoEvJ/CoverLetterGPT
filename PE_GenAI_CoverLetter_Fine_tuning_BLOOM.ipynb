{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PanoEvJ/GenAI-CoverLetter/blob/main/PE_GenAI_CoverLetter_Fine_tuning_BLOOM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE5GJ6s7y0Xo"
      },
      "source": [
        "# Fine-tune a BLOOM-based ad generation model using `peft`, `transformers` and `bitsandbytes`\n",
        "\n",
        "We can use the [job_postings_GPT dataset](PanoEvJ/job_postings_GPT) to fine-tune BLOOM to be able to generate marketing emails based off of a product and its description!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gahXSeKIkc3N"
      },
      "source": [
        "### Overview of PEFT and LoRA:\n",
        "\n",
        "Based on some awesome new research [here](https://github.com/huggingface/peft), we can leverage techniques like PEFT and LoRA to train/fine-tune large models a lot more efficiently. \n",
        "\n",
        "It can't be explained much better than the overview given in the above link: \n",
        "\n",
        "```\n",
        "Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of\n",
        "pre-trained language models (PLMs) to various downstream applications without \n",
        "fine-tuning all the model's parameters. Fine-tuning large-scale PLMs is often \n",
        "prohibitively costly. In this regard, PEFT methods only fine-tune a small \n",
        "number of (extra) model parameters, thereby greatly decreasing the \n",
        "computational and storage costs. Recent State-of-the-Art PEFT techniques \n",
        "achieve performance comparable to that of full fine-tuning.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfBzP8gWzkpv"
      },
      "source": [
        "### Install requirements\n",
        "\n",
        "First, if the enviroment requirements are NOT ALREADY installed from the previous notebook, then run the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otj46qRbtpnd"
      },
      "outputs": [],
      "source": [
        "!python -m pip install -r https://raw.githubusercontent.com/PanoEvJ/GenAI-CoverLetter/main/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOtwYRI3zzXI"
      },
      "source": [
        "### Model loading\n",
        "\n",
        "Let's load the `bloom-1b7` model!\n",
        "\n",
        "We're also going to load the `bigscience/tokenizer` which is the tokenizer for all of the BLOOM models.\n",
        "\n",
        "This step will take some time, as we have to download the model weights which are ~3.44GB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41ILOR1tffEf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cg3fiQOvmI3Q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"bigscience/bloom-1b7\", \n",
        "    torch_dtype=torch.float32,\n",
        "    load_in_8bit=False, \n",
        "    device_map='auto',\n",
        "    # offload_folder='offload'  # activate this argument in case no cuda devices are available and RAM is limited\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/tokenizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fTSZntA1iUG"
      },
      "source": [
        "### Post-processing on the model\n",
        "\n",
        "Finally, we need to apply some post-processing on the 8-bit model to enable training, let's freeze all our layers, and cast the layer-norm in `float32` for stability. We also cast the output of the last layer in `float32` for the same reasons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "T-gy-LxM0yAi"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False  # freeze the model - train adapters later\n",
        "  if param.ndim == 1:\n",
        "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "    param.data = param.data.to(torch.float32)\n",
        "\n",
        "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwOTr7B3NlM3"
      },
      "source": [
        "### Apply LoRA\n",
        "\n",
        "Here comes the magic with `peft`! Let's load a `PeftModel` and specify that we are going to use low-rank adapters (LoRA) using `get_peft_model` utility function from `peft`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4W1j6lxaNnxC"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iwHGzKBN6wk",
        "outputId": "c38ad0a9-9d93-4103-d85f-2762419bf0bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 3145728 || all params: 1725554688 || trainable%: 0.18230242262828822\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model \n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"query_key_value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdjWif4CVXR6"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmB8KTexh4rf"
      },
      "source": [
        "We can simply load our dataset from ðŸ¤— Hugging Face with the `load_dataset` method!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ_HCYruWIHU"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from datasets import load_dataset\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"PanoEvJ/job_postings_GPT\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTuunLuvxEnW"
      },
      "source": [
        "Inspect dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap6OE6R_W2QZ"
      },
      "outputs": [],
      "source": [
        "print(dataset)\n",
        "print(dataset['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "cfe1c2c28bbb4300bb124759f2f3fce7",
            "3be35b7a53fa4fc299f904f7db7312a6",
            "37cebe848afd40fdae2ff37fe499ae7d",
            "2fbea9a9fea34f3594332e49cc8b2498",
            "8da2cabca3d04d0c9e8f08c3d9d7db6b",
            "594ecce2f6dc4fcb8cdd82a15aa59af4",
            "bdacbafb69f144648434a64786da2470",
            "a960b3ea98a644e69f207a8d480dcb2d",
            "08cca997af98452a9d76febbe6c43181",
            "77aef6e3220d43999bdb75adbbdba29b",
            "113d0e16e3f74235ba65b347de9e0f82"
          ]
        },
        "id": "MWZk1U-kXwZF",
        "outputId": "b09545bd-a9b6-4c53-b138-7c7beb2216ab"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/297 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cfe1c2c28bbb4300bb124759f2f3fce7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "def generate_prompt(job: str, letter: str) -> str:\n",
        "  prompt = f\"Below is a job posting, please write a cover letter for this job.\\n\\n### Job:\\n{job}\\n\\n### Letter:\\n{letter}\"\n",
        "  return prompt\n",
        "\n",
        "mapped_dataset = dataset.map(lambda samples: tokenizer(generate_prompt(samples['job_postings'], samples['cover_letters'])))\n",
        "# mapped_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWe9MBT4aT2v"
      },
      "outputs": [],
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=model, \n",
        "    train_dataset=mapped_dataset[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=4, \n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=100,\n",
        "        max_steps=100, \n",
        "        learning_rate=1e-3, \n",
        "        fp16=True,\n",
        "        logging_steps=1, \n",
        "        output_dir='outputs'\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Duak7T_B3VpJ"
      },
      "source": [
        "## Share adapters on the ðŸ¤— Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjHE2wryglHS"
      },
      "outputs": [],
      "source": [
        "HUGGING_FACE_USER_NAME = \"PanoEvJ\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpYr24pR8T_0"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxB6UV5XAvvP"
      },
      "outputs": [],
      "source": [
        "model_name = \"GenAI-CoverLetter\"\n",
        "\n",
        "model.push_to_hub(f\"{HUGGING_FACE_USER_NAME}/{model_name}\", use_auth_token=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S65GcxNGA9kz"
      },
      "source": [
        "## Load adapters from the Hub\n",
        "\n",
        "You can also directly load adapters from the Hub using the commands below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsD1VKqeA62Z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "peft_model_id = f\"{HUGGING_FACE_USER_NAME}/{model_name}\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=False, device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHYljmTjj5wX"
      },
      "source": [
        "## Inference\n",
        "\n",
        "You can then directly use the trained model or the model that you have loaded from the ðŸ¤— Hub for inference as you would do it usually in `transformers`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuppXQXWA27h"
      },
      "source": [
        "### Take it for a spin!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whuXcSPsluc5"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def make_inference(job_posting):\n",
        "  batch = tokenizer(f\"Below is a job posting, please write a cover letter for this product.\\n\\n### Job posting:\\n{job_posting} \\n\\n### Cover letter:\\n\", return_tensors='pt')\n",
        "\n",
        "  with torch.cuda.amp.autocast():\n",
        "    output_tokens = model.generate(**batch, max_new_tokens=200)\n",
        "\n",
        "  display(Markdown((tokenizer.decode(output_tokens[0], skip_special_tokens=True))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        },
        "id": "E5_d4r1Mm6ud",
        "outputId": "97c1b553-0b4d-44e9-922c-6178a95c0dd3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Below is a job posting, please write a cover letter for this product.\n\n### Job posting:\nMachine Learning Engineer\nFull-time Â· Mid-Senior level\n\nVantAI is building artificial intelligence to revolutionize drug discovery and development. We produce technology to help scientists deliver novel compounds for life saving therapies. We were launched by Roivant Sciences, a global biopharma focused on rapidly developing innovative medicines and drug development technologies.\n\nAbout You:\n\nWe are looking for an experienced Machine Learning Engineer/Scientist to join our machine learning team to help develop the worldâ€™s most advanced ML pipeline for the design of proximity inducing molecules. You will work with a team of world-class machine learning engineers in a research-heavy position on a range of unsolved problems around representation learning of proteins, small molecules, biological networks and genomics.\n\n\nKey Responsibilities:\n\nScientifically direct the design and training of large-scale, state-of-the art Deep Learning systems\nDevelop novel architecture and training paradigms to lead the industry in unsolved scientific problems\nCollaborate with content experts from other domains (e.g., chemistry, physics, biology) to enable innovative feature-engineering and novel cross-disciplinary approaches\n\nBasic Requirements:\n\nMS/PhD degree in Computer Science, Statistics, Applied Mathematics, Computational Biology, Computational Chemistry or other related subject (will also consider BS degrees in these areas for candidates highly qualified across all other requirements or with significant work experience)\nTrack record of contributing to novel methods for state-of-the-art Deep Learning (in industry or through publications) including large-scale Transformers, Graph Neural Nets, ConvNets, etc.\n2+ years of experience on machine learning teams, ideally at a start-up\n4+ years of ML research experience in industry or academia, with strong familiarity with PyTorch; familiarity with Jax is a plus\nExperience with Python is required; programming skills in Rust, C, C++ is a plus\nRelevant experience working in Linux/UNIX environment with basic data engineering and scripting abilities\nAbility to understand business problems and how to build models that can quickly drive value, while prioritizing your research efforts accordingly\n\nPreferred Qualifications:\n\nExperience in Cheminformatics, Computational Biology or Computational Chemistry\nCompetitive programming or scientific experience, including Kaggle, PUTNAM, CTFs, iGEM, Biology/Chemistry Olympiad\nStrong working knowledge of containerized production (e.g., Go/Flask-Server running within Docker, Kubernetes), DevOps and CI/CD principles\nExperience with state-of-the-art tools such as TensorFlow, MXNet and Sklearn\nExperience working with large data sets, simulation/optimization, and distributed computing tools (e.g., Spark, Airflow, Dash, etc.)\n \n\n### Cover letter:\nDear Hiring Manager,\n\nI am excited to apply for the position of Machine Learning Engineer at VantAI. As a highly skilled and experienced Machine Learning Scientist, I am confident that I can contribute to the development of the worldâ€™s most advanced ML pipeline for the design of proximity inducing molecules. I am particularly drawn to the opportunity to work with a team of world-class machine learning engineers on a range of unsolved problems around representation learning of proteins, small molecules, biological networks and genomics.\n\nI have a Bachelor's degree in Computer Science and have a track record of contributing to novel methods for state-of-the-art Deep Learning (in industry or through publications). I have also worked on large-scale Transformers, Graph Neural Nets, ConvNets, etc. I am confident that my skills and experience can add value to your team and help you deliver innovative compounds for life saving therapies.\n\nI am particularly impressed with the company's focus on rapidly developing innovative medicines and drug development technologies."
          },
          "metadata": {}
        }
      ],
      "source": [
        "your_job_posting_here = \"\"\"Machine Learning Engineer\n",
        "Full-time Â· Mid-Senior level\n",
        "\n",
        "VantAI is building artificial intelligence to revolutionize drug discovery and development. We produce technology to help scientists deliver novel compounds for life saving therapies. We were launched by Roivant Sciences, a global biopharma focused on rapidly developing innovative medicines and drug development technologies.\n",
        "\n",
        "About You:\n",
        "\n",
        "We are looking for an experienced Machine Learning Engineer/Scientist to join our machine learning team to help develop the worldâ€™s most advanced ML pipeline for the design of proximity inducing molecules. You will work with a team of world-class machine learning engineers in a research-heavy position on a range of unsolved problems around representation learning of proteins, small molecules, biological networks and genomics.\n",
        "\n",
        "\n",
        "Key Responsibilities:\n",
        "\n",
        "Scientifically direct the design and training of large-scale, state-of-the art Deep Learning systems\n",
        "Develop novel architecture and training paradigms to lead the industry in unsolved scientific problems\n",
        "Collaborate with content experts from other domains (e.g., chemistry, physics, biology) to enable innovative feature-engineering and novel cross-disciplinary approaches\n",
        "\n",
        "Basic Requirements:\n",
        "\n",
        "MS/PhD degree in Computer Science, Statistics, Applied Mathematics, Computational Biology, Computational Chemistry or other related subject (will also consider BS degrees in these areas for candidates highly qualified across all other requirements or with significant work experience)\n",
        "Track record of contributing to novel methods for state-of-the-art Deep Learning (in industry or through publications) including large-scale Transformers, Graph Neural Nets, ConvNets, etc.\n",
        "2+ years of experience on machine learning teams, ideally at a start-up\n",
        "4+ years of ML research experience in industry or academia, with strong familiarity with PyTorch; familiarity with Jax is a plus\n",
        "Experience with Python is required; programming skills in Rust, C, C++ is a plus\n",
        "Relevant experience working in Linux/UNIX environment with basic data engineering and scripting abilities\n",
        "Ability to understand business problems and how to build models that can quickly drive value, while prioritizing your research efforts accordingly\n",
        "\n",
        "Preferred Qualifications:\n",
        "\n",
        "Experience in Cheminformatics, Computational Biology or Computational Chemistry\n",
        "Competitive programming or scientific experience, including Kaggle, PUTNAM, CTFs, iGEM, Biology/Chemistry Olympiad\n",
        "Strong working knowledge of containerized production (e.g., Go/Flask-Server running within Docker, Kubernetes), DevOps and CI/CD principles\n",
        "Experience with state-of-the-art tools such as TensorFlow, MXNet and Sklearn\n",
        "Experience working with large data sets, simulation/optimization, and distributed computing tools (e.g., Spark, Airflow, Dash, etc.)\n",
        "\"\"\"\n",
        "\n",
        "make_inference(your_job_posting_here)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "cvAI",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cfe1c2c28bbb4300bb124759f2f3fce7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3be35b7a53fa4fc299f904f7db7312a6",
              "IPY_MODEL_37cebe848afd40fdae2ff37fe499ae7d",
              "IPY_MODEL_2fbea9a9fea34f3594332e49cc8b2498"
            ],
            "layout": "IPY_MODEL_8da2cabca3d04d0c9e8f08c3d9d7db6b"
          }
        },
        "3be35b7a53fa4fc299f904f7db7312a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_594ecce2f6dc4fcb8cdd82a15aa59af4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_bdacbafb69f144648434a64786da2470",
            "value": "Map:  95%"
          }
        },
        "37cebe848afd40fdae2ff37fe499ae7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a960b3ea98a644e69f207a8d480dcb2d",
            "max": 297,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08cca997af98452a9d76febbe6c43181",
            "value": 297
          }
        },
        "2fbea9a9fea34f3594332e49cc8b2498": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77aef6e3220d43999bdb75adbbdba29b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_113d0e16e3f74235ba65b347de9e0f82",
            "value": " 283/297 [00:01&lt;00:00, 224.97 examples/s]"
          }
        },
        "8da2cabca3d04d0c9e8f08c3d9d7db6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "594ecce2f6dc4fcb8cdd82a15aa59af4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdacbafb69f144648434a64786da2470": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a960b3ea98a644e69f207a8d480dcb2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08cca997af98452a9d76febbe6c43181": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "77aef6e3220d43999bdb75adbbdba29b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "113d0e16e3f74235ba65b347de9e0f82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}